{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps = 5\n",
    "batch_size = 200\n",
    "num_classes = 2\n",
    "state_size = 4\n",
    "learning_rate = 0.1\n",
    "logfile = \"./rnns.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_data(size=1000000):\n",
    "    X = np.array(np.random.choice(2, size=(size,)))\n",
    "    Y = []\n",
    "    for i in range(size):\n",
    "        threshold = 0.5\n",
    "        if X[i-3] == 1:\n",
    "            threshold += 0.5\n",
    "        if X[i-8] == 1:\n",
    "            threshold -= 0.25\n",
    "        if np.random.rand() > threshold:\n",
    "            Y.append(0)\n",
    "        else:\n",
    "            Y.append(1)\n",
    "    return X, np.array(Y)\n",
    "\n",
    "# adapted from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/reader.py\n",
    "def gen_batch(raw_data, batch_size, num_steps):\n",
    "    raw_x, raw_y = raw_data\n",
    "    data_length = len(raw_x)\n",
    "\n",
    "    # partition raw data into batches and stack them vertically in a data matrix\n",
    "    batch_partition_length = data_length // batch_size\n",
    "    data_x = np.zeros([batch_size, batch_partition_length], dtype=np.int32)\n",
    "    data_y = np.zeros([batch_size, batch_partition_length], dtype=np.int32)\n",
    "    for i in range(batch_size):\n",
    "        data_x[i] = raw_x[batch_partition_length * i:batch_partition_length * (i + 1)]\n",
    "        data_y[i] = raw_y[batch_partition_length * i:batch_partition_length * (i + 1)]\n",
    "    # further divide batch partitions into num_steps for truncated backprop\n",
    "    epoch_size = batch_partition_length // num_steps\n",
    "\n",
    "    for i in range(epoch_size):\n",
    "        x = data_x[:, i * num_steps:(i + 1) * num_steps]\n",
    "        y = data_y[:, i * num_steps:(i + 1) * num_steps]\n",
    "        yield (x, y)\n",
    "\n",
    "def gen_epochs(n, num_steps):\n",
    "    for i in range(n):\n",
    "        yield gen_batch(gen_data(), batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.int32, [batch_size, None], name='input_placeholder')\n",
    "y = tf.placeholder(tf.int32, [batch_size, None], name='labels_placeholder')\n",
    "init_state = tf.zeros([batch_size, state_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_one_hot = tf.one_hot(x, num_classes)\n",
    "rnn_inputs = [tf.squeeze(i,squeeze_dims=[1]) for i in tf.split(1, num_steps, x_one_hot)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('rnn_cell'):\n",
    "    W = tf.get_variable('W', [num_classes + state_size, state_size])\n",
    "    b = tf.get_variable('b', [state_size], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "def rnn_cell(rnn_input, state):\n",
    "    with tf.variable_scope('rnn_cell', reuse=True):\n",
    "        W = tf.get_variable('W', [num_classes + state_size, state_size])\n",
    "        b = tf.get_variable('b', [state_size], initializer=tf.constant_initializer(0.0))\n",
    "    return tf.tanh(tf.matmul(tf.concat(1, [rnn_input, state]), W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state = init_state\n",
    "rnn_outputs = []\n",
    "for rnn_input in rnn_inputs:\n",
    "    state = rnn_cell(rnn_input, state)\n",
    "    rnn_outputs.append(state)\n",
    "final_state = rnn_outputs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('softmax'):\n",
    "    W = tf.get_variable('W', [state_size, num_classes])\n",
    "    b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "logits = [tf.matmul(rnn_output, W) + b for rnn_output in rnn_outputs]\n",
    "predictions = [tf.nn.softmax(logit) for logit in logits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_as_list = [tf.squeeze(i, squeeze_dims=[1]) for i in tf.split(1, num_steps, y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(logit,label) for \\\n",
    "          logit, label in zip(logits, y_as_list)]\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "train_step = tf.train.AdagradOptimizer(learning_rate).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_network(num_epochs, num_steps, state_size=4, verbose=True):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        training_losses = []\n",
    "        for idx, epoch in enumerate(gen_epochs(num_epochs, num_steps)):\n",
    "            print(\"idx:\" + str(idx))\n",
    "            print(\"epoch:\" + str(epoch))\n",
    "            training_loss = 0\n",
    "            training_state = np.zeros((batch_size, state_size))\n",
    "            if verbose:\n",
    "                print(\"EPOCH\", idx)\n",
    "            for step, (X, Y) in enumerate(epoch):\n",
    "                f_write(str(step), logfile)\n",
    "                f_write(str(X), logfile)\n",
    "                f_write(str(Y), logfile)\n",
    "                tr_losses, training_loss_, training_state, _ = \\\n",
    "                    sess.run([losses,\n",
    "                              total_loss,\n",
    "                              final_state,\n",
    "                              train_step],\n",
    "                                  feed_dict={x:X, y:Y, init_state:training_state})\n",
    "                training_loss += training_loss_\n",
    "                if step % 100 == 0 and step > 0:\n",
    "                    if verbose:\n",
    "                        print(\"Average loss at step\", step,\n",
    "                              \"for last 250 steps:\", training_loss/100)\n",
    "                    training_losses.append(training_loss/100)\n",
    "                    training_loss = 0\n",
    "\n",
    "    return training_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx:0\n",
      "epoch:<generator object gen_batch at 0x10fb7d500>\n",
      "('EPOCH', 0)\n",
      "('Average loss at step', 100, 'for last 250 steps:', 0.610304844379425)\n",
      "('Average loss at step', 200, 'for last 250 steps:', 0.58854409277439113)\n",
      "('Average loss at step', 300, 'for last 250 steps:', 0.58662387907505031)\n",
      "('Average loss at step', 400, 'for last 250 steps:', 0.56168976604938503)\n",
      "('Average loss at step', 500, 'for last 250 steps:', 0.52065922528505326)\n",
      "('Average loss at step', 600, 'for last 250 steps:', 0.51776931077241894)\n",
      "('Average loss at step', 700, 'for last 250 steps:', 0.52038377285003667)\n",
      "('Average loss at step', 800, 'for last 250 steps:', 0.51836444586515429)\n",
      "('Average loss at step', 900, 'for last 250 steps:', 0.51668840199708943)\n"
     ]
    }
   ],
   "source": [
    "training_losses = train_network(1,num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10fe746d0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEACAYAAAC9Gb03AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f835a50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}